{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "# from lib.models.CNN import CNN\n",
    "from lib.utils.modelUtils import ModelUtils\n",
    "from lib.utils.dataUtils import DataUtils\n",
    "from lib.utils.sweep_configs import sweep_config\n",
    "from lib.models.CNN import cnns\n",
    "import random\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from os import environ\n",
    "\n",
    "# Set random seeds\n",
    "environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "random.seed(hash('setting random seeds') % 2**32 - 1)\n",
    "np.random.seed(hash('improves reproducibility') % 2**32 - 1)\n",
    "tf.random.set_seed(hash('so that runs are repeatable'))\n",
    "\n",
    "print(\"# GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    print('GPU Device Name', tf.test.gpu_device_name())\n",
    "\n",
    "LABELS = [\n",
    "    'buildings', \n",
    "    'forest', \n",
    "    'glacier', \n",
    "    'mountain', \n",
    "    'sea', \n",
    "    'street'\n",
    "]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "data_utils = DataUtils('../data', '../outputs')\n",
    "x_train, y_train = data_utils.load_data(train=True)\n",
    "x_test = data_utils.load_data(train=False)\n",
    "\n",
    "# Pre-process\n",
    "x_train = data_utils._normalize(x_train)\n",
    "y_train = to_categorical(\n",
    "    y_train, \n",
    "    len(LABELS)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()\n",
    "model = ModelUtils(\n",
    "    model=cnns['le-net-5'],\n",
    "    training_data=(x_train, y_train)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login()\n",
    "sweep_id = wandb.sweep(sweep_config, entity='manelfideles', project=\"intel-icc\")\n",
    "wandb.agent(\n",
    "    sweep_id, \n",
    "    model.train,\n",
    "    count = 5\n",
    ")\n",
    "wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "We're facing extreme overfitting. Validation results are several times worse than training results.\n",
    "- Use data augmentation: One way to combat overfitting is to use data augmentation. By creating variations of the training data, such as flipping or rotating the images, you can increase the size of the training set and reduce overfitting. In TensorFlow, you can use the ImageDataGenerator class to apply data augmentation.\n",
    "- Add regularization: Regularization is a technique that adds a penalty term to the loss function to discourage overfitting. In TensorFlow, you can add regularization to the convolutional and dense layers using the kernel_regularizer argument. For example, you can use L1 or L2 regularization, or a combination of both.\n",
    "- Reduce the model's complexity: If the model is too complex for the size of the dataset, it can overfit. One way to reduce the complexity of the model is to reduce the number of convolutional or dense layers, or to decrease the number of filters or neurons in each layer.\n",
    "- Increase the dropout rate: Dropout is a technique that randomly drops out some neurons during training to prevent overfitting. If the model is overfitting, you can increase the dropout rate to drop more neurons.\n",
    "- Use early stopping: Early stopping is a technique that stops the training process when the validation loss stops improving. This prevents the model from overfitting to the training data by stopping the training before the model starts to overfit. In TensorFlow, you can use the EarlyStopping callback to implement early stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
