{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for running in Google Colab. It has all the classes and methods together for convenience.\n",
    "To run this notebook, simply upload the following files to the session storage:\n",
    "- trainX.npy\n",
    "- trainy.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install tensorflow wandb numpy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from lib.utils.modelUtils import ModelUtils\n",
    "from lib.utils.dataUtils import DataUtils\n",
    "from lib.utils.sweep_configs import sweep_config\n",
    "from lib.models.CNN import cnns\n",
    "import random\n",
    "import numpy as np\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from os import environ\n",
    "\n",
    "# Set random seeds\n",
    "environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "random.seed(hash('setting random seeds') % 2 ** 32 - 1)\n",
    "np.random.seed(hash('improves reproducibility') % 2 ** 32 - 1)\n",
    "tf.random.set_seed(hash('so that runs are repeatable'))\n",
    "\n",
    "print(\"# GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "if len(tf.config.list_physical_devices('GPU')):\n",
    "    print('GPU Device Name', tf.test.gpu_device_name())\n",
    "\n",
    "LABELS = ['buildings', 'forest', 'glacier', 'mountain', 'sea', 'street']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "cnns = {\n",
    "    'simple-net': Sequential([\n",
    "        Conv2D(32, 3, padding='same', input_shape=(50, 50, 3), activation='relu'),\n",
    "        MaxPooling2D(),\n",
    "        Flatten(),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(units=6, activation='softmax')\n",
    "    ]),\n",
    "    'conv-conv-mp-flat-ds-ds-do-ds': Sequential([\n",
    "        Conv2D(16, 3, padding='same', input_shape=(50, 50, 3), activation='relu'),\n",
    "        Conv2D(16, 5, padding='same', activation='relu'),\n",
    "        MaxPooling2D(2),\n",
    "        Flatten(),\n",
    "        Dense(100, activation='relu'),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(rate=0.6),\n",
    "        Dense(units=6, activation='softmax')\n",
    "    ]),\n",
    "    'conv-conv-mp-f-ds-do-ds-do-ds': Sequential([\n",
    "        Conv2D(16, 3, strides=1, padding=\"same\", input_shape=(50, 50, 3), activation='relu'),\n",
    "        Conv2D(32, 3, strides=1, padding=\"same\", activation='relu'),\n",
    "        MaxPooling2D(2),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(rate=0.6),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(rate=0.5),\n",
    "        Dense(units=6, activation='softmax')\n",
    "    ]),\n",
    "    'le-net-5': Sequential([\n",
    "        Conv2D(filters=6, kernel_size=(5, 5), activation='relu', input_shape=(50, 50, 3)),\n",
    "        AveragePooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(filters=16, kernel_size=(5, 5), activation='relu'),\n",
    "        AveragePooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(units=120, activation='relu'),\n",
    "        Dense(units=84, activation='relu'),\n",
    "        Dense(units=6, activation='softmax')\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLP Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweep Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any\n",
    "\n",
    "sweep_config: Dict[str, Any] = {\n",
    "    'method': 'random',\n",
    "    'metric': {\n",
    "        'goal': 'minimize', \n",
    "        'name': 'loss'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'optimizer': {\n",
    "            'values': ['adam', 'sgd']\n",
    "        },\n",
    "        'epochs': { 'value': 10 },\n",
    "        'learning_rate': {\n",
    "            'values': [0.1, 0.01, 0.001, 0.0001]\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "class ModelUtils:\n",
    "    '''\n",
    "    Utility methods for model training, \n",
    "    validation, testing and selection.\n",
    "    '''\n",
    "    def __init__(self, model, training_data, validation_data, batch_size):\n",
    "        self.model = model\n",
    "        self.training_data = training_data\n",
    "        self.validation_data = validation_data\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def build_optimizer(self, optimizer: str = 'adam', lr: float = 0.01, momentum: float = 0.0):\n",
    "        if optimizer == 'adam':\n",
    "            return tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        else:\n",
    "            return tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum)\n",
    "    \n",
    "    def build_model(self, config):\n",
    "        '''\n",
    "        Builds necessary models components according to \n",
    "        config and compiles them into self.model.\n",
    "        :param config: config passed from config file\n",
    "        '''\n",
    "        optimizer = self.build_optimizer(config.optimizer, config.learning_rate, 0.9)\n",
    "        loss = tf.keras.losses.CategoricalCrossentropy(from_logits = False)\n",
    "        metrics = ['Accuracy', 'AUC', 'Precision', 'Recall']\n",
    "        self.model.compile(optimizer, loss=loss, metrics=metrics)\n",
    "        print(self.model.summary())\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        This method trains self.model. \n",
    "        Performs validation in an embedded manner.\n",
    "        @TODO Add cross-validation ?\n",
    "        :param x: training data\n",
    "        :param y: training labels\n",
    "        '''\n",
    "        config_defaults = {\n",
    "            'optimizer': 'adam',\n",
    "            'batch_size': 512,\n",
    "            'learning_rate': 0.01,\n",
    "            'epochs': 10,\n",
    "        }\n",
    "        wandb.init(project='intel-icc', entity='manelfideles', config=config_defaults)\n",
    "        config = wandb.config\n",
    "        self.build_model(config)\n",
    "        self.model.fit(\n",
    "            self.training_data,\n",
    "            validation_data=self.validation_data,\n",
    "            epochs=config.epochs, \n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[WandbCallback()]\n",
    "        )\n",
    "        wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image as im\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class DataUtils:\n",
    "    '''\n",
    "    Utility methods for importing and exporting data, as well\n",
    "    as for simple pre-processing.\n",
    "    '''\n",
    "    def __init__(self, input_dir_path: str, output_dir_path: str, batch_size: int = 512):\n",
    "        self.input_dir_path = input_dir_path\n",
    "        self.output_dir_path = output_dir_path\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def load_npy(self, filename: str) -> np.array:\n",
    "        return np.load(\n",
    "            self.input_dir_path + f'/{filename}.npy',\n",
    "            allow_pickle=True\n",
    "        )\n",
    "\n",
    "    def tvt_split(self, x, y, test_size: float = 0.3):\n",
    "        x_train, x_test, y_train, y_test = train_test_split(\n",
    "            x, y,\n",
    "            test_size=test_size, \n",
    "            random_state=1\n",
    "        )\n",
    "        return x_train, x_test, y_train, y_test\n",
    "\n",
    "    def make_image_generator(self, train: bool = True):\n",
    "        return ImageDataGenerator(\n",
    "            rescale = 1./255,\n",
    "            rotation_range = 20,\n",
    "            width_shift_range = 0.2,\n",
    "            height_shift_range = 0.2,\n",
    "            horizontal_flip = True,\n",
    "            validation_split = 0.2\n",
    "        )\n",
    "    \n",
    "    def load_data(self, train: bool):\n",
    "        if train:\n",
    "            x, y = self.load_npy('trainX'), to_categorical(self.load_npy('trainy'))\n",
    "            x_train, x_val, y_train, y_val = self.tvt_split(x, y)\n",
    "            train_datagen = self.make_image_generator()\n",
    "            train_datagen.fit(x_train)\n",
    "            std_datagen = ImageDataGenerator(\n",
    "                preprocessing_function=train_datagen.standardize\n",
    "            )\n",
    "            train_data = train_datagen.flow(\n",
    "                x_train,\n",
    "                y_train,\n",
    "                batch_size=self.batch_size,\n",
    "            )\n",
    "            val_data = std_datagen.flow(x_val, y_val, batch_size=self.batch_size)\n",
    "            return (train_data, val_data)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def save_model(self, filename: str):\n",
    "        raise NotImplementedError   \n",
    "\n",
    "    def display_image(self, array: np.array) -> im.Image: \n",
    "        return im.fromarray(array)\n",
    "    \n",
    "    def _normalize(self, array: np.array) -> np.array:\n",
    "        return (array / 255)\n",
    "    \n",
    "    def _standardize(self, array: np.array) -> np.array:\n",
    "        mean, std = np.mean(\n",
    "            array, \n",
    "            axis=(1,2), \n",
    "            keepdims=True\n",
    "        ), np.std(\n",
    "            array,\n",
    "            axis=(1,2), \n",
    "            keepdims=True\n",
    "        )\n",
    "        return ((array - mean) / std)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data_utils = DataUtils('./data', './outputs')\n",
    "train_data, val_data = data_utils.load_data(train = True)\n",
    "\n",
    "# Instatiate the model\n",
    "K.clear_session()\n",
    "model = ModelUtils(\n",
    "    model = cnns['simple-net'],\n",
    "    training_data = train_data,\n",
    "    validation_data = val_data,\n",
    "    batch_size = 512\n",
    ")\n",
    "\n",
    "# Run sweeps and log results\n",
    "wandb.login()\n",
    "sweep_id = wandb.sweep(sweep_config, entity='manelfideles', project=\"intel-icc\")\n",
    "wandb.agent(\n",
    "    sweep_id, \n",
    "    model.train,\n",
    "    count = 5\n",
    ")\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
